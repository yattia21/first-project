<h1> <u> Ethical Reflection: Machine Bias </u> </h1>
<p>
The ProPublica article Machine Bias describes the usage of the software COMPAS in the justice system, and how according to one of their studies the algorithm’s mistakes were in favor of white defendants. The article states, “The formula was particularly likely to falsely flag black defendants as future criminals, wrongly labeling them this way at almost twice the rate as white defendants”, adding that “white defendants were mislabeled as low risk more often than black defendants” (Machine Bias: https://www.propublica.org ).
Although some of these court sentences may be life changing, it seems like officials put their full trust in this software and blindly follow its results. It was quite jarring to read the several examples Machine Bias provided, since it shows how often this injustice affects these offenders.
 For example, in Bernard Parker’s case, he received a 10 on the risk assessment scale, most likely because of what is taken into account in the probability algorithm (past crimes of parents, friends’ drug use, anger issues, etc.). These aspects of an offender’s life do not seem like sufficient information to come to a conclusion on the likelihood of their risk probability, especially when certain criminals may come from an environment where drug use and violence may be prevelant. Offender profiling based on backgrounds has proven to be moderately useful at best by this study which proves there obviously needs to be a change in the justice system.  Napa County Superior Court Judge Mark Boessenecker says it best: “A guy who has molested a small child every day for a year could still come out as a low risk because he probably has a job. Meanwhile, a drunk guy will look high risk because he’s homeless. These risk factors don’t tell you whether the guy ought to go to prison or not; the risk factors tell you more about what the probation conditions ought to be.”
 </p>
